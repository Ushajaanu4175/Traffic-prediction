import pandas as pd
import torch
import torch.optim as optim
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from gnn_lstm_model import TemporalCNN
from preprocess_data import preprocess_data
from torch_lr_finder import LRFinder  # ‚úÖ Import Learning Rate Finder

# ‚úÖ Load dataset
df = pd.read_csv('data/traffic_volume_data.csv')

# ‚úÖ Preprocess data
X, y = preprocess_data(df, fit_scalers=True)

# ‚úÖ Convert to PyTorch tensors
X = torch.tensor(X, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32)

# ‚úÖ Create DataLoader (Fixed batch size)
batch_size =  128
 # Reduced batch size to allow more LR finder iterations
dataset = TensorDataset(X, y)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
print(f"‚úÖ Training Features Count: {X.shape[1]}")

# ‚úÖ Initialize Model
input_dim = X.shape[1]
model = TemporalCNN(input_dim=input_dim, dropout=0.5)

# ‚úÖ Use CPU (since you don't have a GPU)
device = torch.device("cpu")
model.to(device)

# ‚úÖ RMSprop Optimizer
optimizer = optim.RMSprop(model.parameters(), lr=0.00159, alpha=0.9, eps=1e-8, weight_decay=1e-4)

# ‚úÖ Loss Function (Mean Squared Error)
criterion = nn.MSELoss()

# ‚úÖ Learning Rate Finder
lr_finder = LRFinder(model, optimizer, criterion, device=device)

# ‚úÖ Find Best Learning Rate (Fixed issue with train_loader)
lr_finder.range_test(dataloader, end_lr=0.1, num_iter=100)  # Increased num_iter
lr_finder.plot()  # ‚úÖ Check the plot and update LR
lr_finder.reset()

# ‚úÖ Learning Rate Scheduler (Cosine Annealing)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)

# ‚úÖ Early Stopping Variables
best_loss = float('inf')
patience = 10
patience_counter = 0

# ‚úÖ Training Loop
epochs = 100
for epoch in range(epochs):
    model.train()  # Ensure model is in training mode
    epoch_loss = 0
    
    for batch_X, batch_y in dataloader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)  # Ensure data is on CPU

        optimizer.zero_grad()
        
        # ‚úÖ Forward Pass
        output = model(batch_X)
        
        # ‚úÖ Compute Loss
        loss = criterion(output, batch_y)
        loss.backward()
        
        # ‚úÖ Gradient Clipping (Prevents Exploding Gradients)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
        
        # ‚úÖ Optimizer Step
        optimizer.step()
        
        # ‚úÖ Track Epoch Loss
        epoch_loss += loss.item()

    # ‚úÖ Compute Average Loss
    avg_loss = epoch_loss / len(dataloader)

    # ‚úÖ Learning Rate Decay
    scheduler.step()

    # ‚úÖ Early Stopping Check
    if avg_loss < best_loss:
        best_loss = avg_loss
        patience_counter = 0
        torch.save(model.state_dict(), "models/best_model.pth")  # Save Best Model
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print(f"‚èπ Early stopping at epoch {epoch+1}. Best Loss: {best_loss:.6f}")
            break

    # ‚úÖ Print Training Progress
    print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}, LR: {scheduler.get_last_lr()[0]:.6f}")

print("üéØ Training Complete! Best Model Saved.")
